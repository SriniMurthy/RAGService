rag:
  documents:
    path: "classpath:documents/*"
    watch-dir: ${RAG_DROP_FOLDER}
  prompt:
    template: |
      You are a helpful AI assistant that analyzes documents to answer questions accurately.

      CRITICAL REASONING INSTRUCTIONS:
      1. Read ALL provided DOCUMENTS carefully before answering
      2. When questions mention specific timeframes, numbers, or entities:
         - Look for date ranges, time periods, or numeric ranges that CONTAIN the queried value
         - Example: A range "2008-2017" contains the year "2014"
         - Example: A range "$100K-$200K" contains "$150K"
      3. When questions ask for comparisons or changes over time:
         - Identify multiple time periods in the documents
         - Compare information across those periods
      4. Synthesize information across MULTIPLE document chunks if needed
      5. If information spans multiple documents, combine them coherently

      ANSWER GUIDELINES:
      - Provide detailed, specific answers citing information from the DOCUMENTS
      - If the answer is not available in the DOCUMENTS, clearly state:
        "I don't have enough information in the provided documents to answer this question."
      - Do NOT make assumptions or add information not present in DOCUMENTS

      QUESTION:
      {input}

      DOCUMENTS:
      {documents}
spring:
  codec:
    max-in-memory-size: 25MB
  application:
    name: RAGJava
  profiles:
    active: data-ingestion
  datasource:
    url: jdbc:postgresql://localhost:5432/RAGJava
    username: ${SM_USER_NAME}
    password: ${SM_PASSWORD}
    driver-class-name: org.postgresql.Driver
  ai:
    openai:
      api-key: ${OPENAI_API_KEY}
      embedding:
        options:
          retry-template: '#{retryTemplate}'
      chat:
        options:
          model: gpt-4o
          retry-template: '#{retryTemplate}'
    anthropic:
      api-key: ${ANTHROPIC_API_KEY}
      chat:
        options:
          model: claude-3-haiku-20240307
          temperature: 0.7
    vectorstore:
      pgvector:
        schema-name: public
        initialize-schema: false
  docker:
    compose:
      lifecycle-management: start_only

management:
  endpoints:
    web:
      exposure:
        include: "*"
  endpoint:
    shutdown:
      enabled: true

server:
  servlet:
    context-path: /RAG

pdf:
  reader:
    smart: true              # Enable SmartPdfReader
    enable-ocr: true         # Enable Tika/Tesseract fallback
    min-chars-per-page: 50   # Threshold to detect scanned PDFs

# ===== LANGCHAIN4J AGENT CONFIGURATION =====
langchain4j:
  chat-model:
    provider: open-ai
  open-ai:
    chat-model:
      api-key: ${OPENAI_API_KEY}
      model-name: gpt-4o
      max-tokens: 500
      temperature: 0.3

app:
  ingestion:
    batch-size: 50
    parallel-embeddings: 5
  retrieval:
    # Hybrid retrieval configuration
    topK: 3                          # Reduced from 5 to 3 to stay under 30K token limit
    similarityThreshold: 0.60        # Raised from 0.30 - better precision with hybrid retrieval
    candidateMultiplier: 3           # Reduced from 4 to 3 (retrieves 3 * 3 = 9 candidates)

# ===== REAL-TIME FINANCE APIs =====
finance:
  # Agentic Provider Selection (LLM-powered intelligent routing)
  agentic-selection:
    enabled: true  # Set to true to enable LLM-based provider selection
    fallback-on-failure: true  # Fallback to traditional priority-based if agentic selection fails

  # Alpha Vantage (25 requests/day free tier)
  # Get free key at: https://www.alphavantage.co/support/#api-key
  alpha-vantage:
    enabled: true  # Set to true and add your key
    priority: 10 # Highest priority for real-time data
    key: demo
    base-url: https://www.alphavantage.co

  # Finnhub (60 requests/minute free tier = 3600/hour, 86,400/day)
  # Get free key at: https://finnhub.io/register
  # RECOMMENDED: Much higher limits than Alpha Vantage (MOST LIBERAL PROVIDER)
  finnhub:
    enabled: true  # Set to true and add your key
    priority: 20 # Second priority, good for fundamentals
    key: ${FINNHUB_API_KEY}

  # Yahoo Finance (Web scraping, no key needed)
  yahoo:
    enabled: true
    priority: 100 # Low priority fallback

  # Google Finance (Web scraping, no key needed)
  google:
    enabled: true
    priority: 110 # Lowest priority fallback

logging:
  level:
    org.apache.pdfbox.pdmodel.font.FileSystemFontProvider: ERROR

# ===== WEATHER SERVICE CONFIGURATION =====
# External microservice for weather data (OpenWeatherMap)
weather:
  service:
    url: ${WEATHER_SERVICE_URL:http://localhost:8081}

# ===== RAFT DATASET GENERATION CONFIGURATION =====
raft:
  # Maximum number of concurrent LLM API calls to prevent rate limiting
  # OpenAI free tier: ~3 requests/min, Tier 1: 30K tokens/min
  # Adjust based on your OpenAI tier and token usage per request
  # Recommended: 5-10 for Tier 1, 2-3 for free tier
  max-concurrent-llm-calls: 10

# ===== REACT AGENT GUARDRAILS =====
# Prevent runaway costs and infinite loops in ReACT agent
react:
  # Maximum iterations the agent can perform (HARD LIMIT - not overridable by request)
  # REDUCED to 2 for better performance - most queries need only 1 iteration
  max-iterations: 2

  # Maximum tokens allowed per query (estimated)
  # GPT-4o: ~$0.0025/1K input tokens, ~$0.01/1K output tokens
  # 10K tokens â‰ˆ $0.10 max cost per query
  max-tokens-per-query: 10000

  # Stuck detection: If same tool called N times in a row, stop
  max-repeated-tool-calls: 2

  # Maximum cost per query in USD (safety net)
  max-cost-per-query: 0.50

  # Enable cost tracking and logging
  enable-cost-tracking: true

  # Delay between iterations in milliseconds (rate limiting)
  # Helps avoid hitting OpenAI's TPM (tokens per minute) limits
  # Default: 1000ms (1 second)
  iteration-delay-ms: 1000

# ===== MCP (MODEL CONTEXT PROTOCOL) CONFIGURATION =====
# External knowledge sources and tools via MCP servers
# DISABLED: Awaiting stable SDK API compatibility with Spring AI 1.0.3

#  ai:
#    mcp:
#      client:
#        tool-callbacks-enabled: true
#        clients:
#          wikipedia:
#            type: stdio
#            transport:
#              stdio:
#                command: npx
#                args:
#                  - "-y"
#                  - "@modelcontextprotocol/server-wikipedia"

# ===== DYNAMODB CHAT MEMORY CONFIGURATION =====
# Persistent chat history using DynamoDB Local (or AWS DynamoDB)
chat:
  memory:
    dynamodb:
      # Enable DynamoDB-backed chat memory (set to false to use in-memory)
      enabled: true

      # DynamoDB Local endpoint (Docker container on port 8000)
      # Remove this line to use AWS DynamoDB instead
      endpoint: http://localhost:8000

      # AWS Region (required even for DynamoDB Local)
      region: us-west-2

      # Table name for storing chat history
      table-name: chat_history

      # TTL in days - auto-delete conversations older than X days
      # Set to 0 for no expiration (messages kept indefinitely)
      ttl-days: 7
